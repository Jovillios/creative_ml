{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTY-Msjal7zU"
   },
   "source": [
    "# Creative Machine Learning - Deep learning\n",
    "\n",
    "### Author: Philippe Esling (esling@ircam.fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3M5ezRyrjA3"
   },
   "source": [
    "In this course we will cover\n",
    "1. A brief introduction to [deep learning](#learning) and problems that we face\n",
    "2. A formal presentation of [Auto-Encoders](#ae) (AEs)\n",
    "3. An explanation of how to [implement AEs](#implement) and [layer-wise pretraining](#implement)\n",
    "4. An [more modern application](#application) of AEs as generative models\n",
    "4. An practical exemple of [convolutional denoising AEs](#denoising) for image data **(exercise)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "\n",
    "Deep learning is currently the most successful subfield of machine learning that deals with algorithms inspired by the structure and function of the brain through artificial neural networks. It enables computers to learn from vast amounts of data and has revolutionized various fields such as computer vision, natural language processing and speech recognition.\n",
    "\n",
    "## Why deep architectures?\n",
    "Deep architectures are essential because they allow models to learn hierarchical representations from raw input data. These hierarchical representations can capture increasingly complex and abstract features as the network depth increases. This capability enables deep learning models to achieve state-of-the-art results in many tasks, as they can automatically learn to extract relevant features from the input data without relying on hand-engineered features.\n",
    "\n",
    "**References:** \n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). [Deep Learning](http://www.deeplearningbook.org). MIT Press.\n",
    "\n",
    "## Challenges in deep architectures\n",
    "\n",
    "As seen in the course, there are several impediments in learning deep architectures, namely\n",
    "1. Vanishing gradient\n",
    "2. Overfitting issues\n",
    "3. Multiple local minima\n",
    "4. Instability in the training\n",
    "Here, we briefly discuss the two major problems, namely _vanishing gradient_ and _overfitting_.\n",
    "\n",
    "#### Vanishing gradient\n",
    "One of the main challenges in training deep architectures is the vanishing gradient problem. It occurs when gradients of the loss function with respect to the model parameters become too small as they backpropagate through the layers. This issue leads to slow convergence and poor performance, as the weights in the earlier layers do not get updated significantly during training.\n",
    "\n",
    "#### Overfitting\n",
    "Another challenge in deep learning is overfitting, where the model learns to perform well on the training data but fails to generalize to unseen data. Regularization techniques, such as L1 and L2 regularization, dropout, and weight decay, are used to prevent overfitting by adding constraints to the model or by making the training process more robust.\n",
    "\n",
    "**References:**\n",
    "- Pascanu, R., Mikolov, T., & Bengio, Y. (2013). [On the difficulty of training recurrent neural networks](https://proceedings.mlr.press/v28/pascanu13.html). In International conference on machine learning (pp. 1310-1318).\n",
    "- Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). [Dropout: A simple way to prevent neural networks from overfitting](http://jmlr.org/papers/v15/srivastava14a.html). Journal of Machine Learning Research, 15(1), 1929-1958.\n",
    "\n",
    "### Layer-wise Pretraining\n",
    "To address the vanishing gradient problem and improve the training of deep architectures, layer-wise pretraining was proposed. In this approach, each layer of the deep architecture is pretrained independently, usually with unsupervised learning methods like autoencoders, before fine-tuning the entire network using supervised learning.\n",
    "\n",
    "**References:**\n",
    "- Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). [A fast learning algorithm for deep belief nets](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf). Neural computation, 18(7), 1527-1554.\n",
    "- Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P. A. (2008). [Extracting and composing robust features with denoising autoencoders](https://dl.acm.org/doi/10.1145/1390156.1390294). In Proceedings of the 25th international conference on Machine learning (pp. 1096-1103).\n",
    "\n",
    "\n",
    "#### Autoencoders\n",
    "_Autoencoders_ are unsupervised learning models that learn to reconstruct their input data. They consist of an encoder that maps the input data to a lower-dimensional representation and a decoder that reconstructs the input data from the lower-dimensional representation. Autoencoders can be used to learn useful features from the data and initialize the weights of a deep architecture during layer-wise pretraining. As we have seen, an interesting variant of autoencoders are _denoising autoencoders_ that learn to reconstruct the input data from a corrupted version of it. By learning to recover the original data from the corrupted version, denoising autoencoders can capture more robust and meaningful features. They can be used in layer-wise pretraining to improve the training of deep architectures further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief refresher in Pytorch\n",
    "\n",
    "As we have seen previously, `Pytorch` is a Python-based scientific computing package targeted at deep learning, which provides a very large flexibility and easeness of use for GPU calculation. `Pytorch` is constructed around the concept of `Tensor`, which is very similar to `numpy.ndarray`, but can be seamlessly run on GPU. Here, we provide a more detailed refresher on concepts of `Pytorch`.\n",
    "\n",
    "Here are some examples of different `Tensor` creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Create a 5 x 3 Tensor of zeros\n",
    "x = torch.empty(5, 3)\n",
    "# Create a 64 x 3 x 32 x 32 random Tensor\n",
    "x = torch.rand(64, 3, 32, 32)\n",
    "# Create a Tensor of zeros with _long_ type\n",
    "x = torch.zeros(10, 10, dtype=torch.long)\n",
    "# Construct a Tensor from the data\n",
    "x = torch.tensor([5.5, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or create a tensor based on an existing tensor. These methods\n",
    "will reuse properties of the input tensor, e.g. dtype, unless\n",
    "new values are provided by user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n",
      "8\n",
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(8, 2, dtype=torch.double)      # new_* methods take in sizes\n",
    "y = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x.size())\n",
    "print(x.shape[0])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arithmetic operations\n",
    "\n",
    "Tensors provide access to a transparent library of arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7477, 1.2584],\n",
      "        [0.9432, 0.3611],\n",
      "        [0.0801, 1.2151],\n",
      "        [0.1733, 0.0271],\n",
      "        [0.2617, 0.5104],\n",
      "        [1.4543, 1.0331],\n",
      "        [0.7956, 0.3846],\n",
      "        [0.0364, 0.7126]])\n",
      "tensor([[2.7633, 0.2815, 1.4214, 1.2141],\n",
      "        [1.5757, 0.1368, 0.6598, 0.5301],\n",
      "        [1.9585, 0.2260, 1.1757, 1.0416],\n",
      "        [1.0640, 0.0741, 0.3293, 0.2327],\n",
      "        [1.4412, 0.1739, 0.9135, 0.8186],\n",
      "        [2.2133, 0.2079, 1.0270, 0.8525],\n",
      "        [1.4529, 0.1360, 0.6710, 0.5562],\n",
      "        [1.1106, 0.1454, 0.7765, 0.7090]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(8, 2)\n",
    "y = torch.rand(8, 2)\n",
    "z = torch.rand(2, 4)\n",
    "# Equivalent additions\n",
    "a = (x + y)\n",
    "b = (torch.add(x, y))\n",
    "# Add in place\n",
    "x.add_(y)\n",
    "# Put in target Tensor\n",
    "result = torch.empty(8, 2)\n",
    "torch.add(x, y, out=result)\n",
    "# Element_wise multiplication\n",
    "print(x * y)\n",
    "# Matrix product\n",
    "print(x @ z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing and resizing\n",
    "\n",
    "You can slice tensors using the usual Python operators. For resizing and reshaping tensor, you can use ``torch.view`` or ``torch.reshape``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "torch.Size([2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "#print(x[:, 1])\n",
    "import torch\n",
    "x = torch.zeros(16, 32, 32)\n",
    "x2 = torch.zeros(64, 32, 32)\n",
    "y = x.view(-1, 32, 32)\n",
    "y2 = x2.view(-1, 32, 32)\n",
    "print(y.size())\n",
    "print(y2.size())\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())\n",
    "\n",
    "z = torch.rand(4, 4)\n",
    "z = z.view(-1, 2, 2, 2)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a one element tensor, use ``.item()`` to get the value as a\n",
    "Python number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0835])\n",
      "0.0835312083363533\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have more than **100 operations**, including _transposing, indexing, slicing, mathematical operations, linear algebra, random numbers_, which are all described at [https://pytorch.org/docs/torch](https://pytorch.org/docs/torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy bridge\n",
    "\n",
    "Converting a Torch Tensor to a Numpy array and vice versa is extremely simple. Note that the Pytorch Tensor and Numpy array **will share their underlying memory locations** (if the Tensor is on CPU), and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going GPU\n",
    "\n",
    "Tensors can be moved onto any device using the ``.to`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graphs\n",
    "\n",
    "The concept of a computation graph is essential to efficient deep learning programming, because it allows you to not have to write the back propagation gradients yourself. A computation graph is simply a specification of how your data is combined to give you the output (the forward pass). Since the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives. \n",
    "\n",
    "The fundamental flag ``requires_grad`` allows to specify which variables are going to need differentiation in all these operations. If ``requires_grad=True``, the Tensor object keeps track of how it was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x12d82b970>\n",
      "tensor([0.3333, 0.3333, 0.3333])\n"
     ]
    }
   ],
   "source": [
    "# Tensor factory methods have a ``requires_grad`` flag\n",
    "x = torch.tensor([1., 2., 3], requires_grad=True)\n",
    "# With requires_grad=True, we can still do all the operations \n",
    "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
    "print(x)\n",
    "z = x + y\n",
    "out = z.mean(-1)\n",
    "print(z)\n",
    "# But z now knows something extra.\n",
    "print(z.grad_fn)\n",
    "out.backward()\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, `z` knows that it is the direct result of an addition. Furthermore, if we keep following z.grad_fn, we can even find back both `x` and `y`. But how does that help us compute a gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x10da87e20>\n",
      "<AddBackward0 object at 0x10dd35670>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)\n",
    "print(z.grad_fn)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, what is the derivative of this sum with respect to the first\n",
    "component of x? In math, we want\n",
    "\n",
    "\\begin{align}\\frac{\\partial s}{\\partial x_0}\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "Well, s knows that it was created as a sum of the tensor z. z knows\n",
    "that it was the sum x + y. So\n",
    "\n",
    "\\begin{align}s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\\end{align}\n",
    "\n",
    "And so s contains enough information to determine that the derivative we want is 1. We can have Pytorch compute the gradient, and see that we were right:\n",
    "\n",
    "**Note** : If you run this block multiple times, the gradient will increment. That is because Pytorch *accumulates* the gradient into the .grad property, since for many models this is very convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3333, 1.3333, 1.3333])\n"
     ]
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what is going on in the block below is crucial for being a\n",
    "successful programmer in deep learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "None\n",
      "<AddBackward0 object at 0x10dbfb580>\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "# By default, user created Tensors have ``requires_grad=False``\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print(z.grad_fn)\n",
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(z.requires_grad)\n",
    "# Now z has the computation history, which we can **detach**\n",
    "new_z = z.detach()\n",
    "# Which means that we have no gradient attached anymore\n",
    "print(new_z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on Tensors\n",
    "with ``.requires_grad=True`` by wrapping the code block in\n",
    "``with torch.no_grad():``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining networks \n",
    "\n",
    "Here, we briefly recall that in `PyTorch`, the `nn` package provides higher-level abstractions over raw computational graphs that are useful for building neural networks. The `nn` package defines a set of `Modules`, which are roughly equivalent to neural network layers. A `Module` receives input `Tensors` and computes output `Tensors`, but may also hold internal state such as `Tensors` containing learnable parameters. In the following example, we use the `nn` package to show how easy it is to instantiate a three-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Define the input dimensions\n",
    "in_size = 1000\n",
    "# Number of neurons in a layer\n",
    "hidden_size = 100\n",
    "# Output (target) dimension\n",
    "output_size = 10\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(in_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_size, output_size),\n",
    "    nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the slides, we can as easily mix between pre-defined modules and arithmetic operations. Here, we will define our very own *residual* layer, and then combine them in a more complex network. To do so, we first define our own `ResidualLayer` (by subclassing `nn.Module`). This type of layer can now be combined inside a full network by simply using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, dim, dim_res=32):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim_res, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(dim_res, dim, 1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform main residual action\n",
    "        return x + self.block(x)\n",
    "\n",
    "model = nn.Sequential(\n",
    "\tResidualLayer(64, 32),\n",
    "\tResidualLayer(64, 32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders and layer-wise training\n",
    "\n",
    "Here, we discuss the implementation of a deep autoencoder using PyTorch, focusing on layer-wise training. Autoencoders are unsupervised learning models that can learn useful features from the input data. They consist of an encoder, which maps the input data to a lower-dimensional representation, and a decoder, which reconstructs the input data from the lower-dimensional representation. _Deep autoencoders_ are multi-layer autoencoders that can learn more abstract and complex features from the input data. To learn such models, _layer-wise training_ was historically used as a technique to help overcome the vanishing gradient problem and initializes the network with better weights, leading to improved performance. Hence, we aim to _pretrain each layer individually_ in an **unsupervised** manner, followed by fine-tuning the entire network with _supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generative\"></a>\n",
    "## Types of learning\n",
    "\n",
    "### Supervised refresher\n",
    "\n",
    "Until now, we have mostly discussed models that are developed for _supervised_ and _discriminative_ tasks. To formalize this problem, we have a set of data $\\{\\mathbf{x}_{i}, \\mathbf{y}_{i}\\}_{i\\in[1,n]}$, where the $\\mathbf{x}_{i}$ and $\\mathbf{y}_{i}$ are linked. Therefore, we want to approximate this relation through\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\hat{y}} = \\mathcal{F}_\\mathbf{\\theta}(\\mathbf{x})  \n",
    "\\end{equation}\n",
    "where we train the parameters $\\mathbf{\\theta}$ so that $\\mathbf{\\hat{y}}\\approx\\mathbf{y}$. The existence of a label $\\mathbf{y}$ (\"correct answer\") defines a _supervised_ problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going unsupervised\n",
    "\n",
    "In some cases, we might only have a set of data $\\{\\mathbf{x}_{i}\\}_{i\\in[1,n]}$, and still be interested in learning some underlying properties or structure of this set. In that case, the problem is _unsupervised_ as we have to learn without a given answer. \n",
    "\n",
    "Here, we can turn to _generative_ models [[1](#reference1)], which allows to create new data instances based on the observation of existing examples. Although these models are more naturally defined in a _probabilistic way_ (as we will see later when we dwelve in generative models), we will assume that we have some simple _code_ $\\mathbf{z}$, which allows to control the properties of the generation, and need to learn\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\hat{x}} = \\mathcal{F}_\\mathbf{\\theta}(\\mathbf{z})  \n",
    "\\end{equation}\n",
    "where we still need to learn $\\mathbf{\\theta}$, so that $\\mathbf{\\hat{x}}$ have similar properties to that of the examples in $\\{\\mathbf{x}_{i}\\}_{i\\in[1,n]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kg83oWxMm92f"
   },
   "source": [
    "<a id=\"ae\"></a>\n",
    "\n",
    "## Auto-encoders\n",
    "\n",
    "One way to understand a set of data is to try to _compress_, or _simplify_ the corresponding dataset. So the idea is to learn simultaneously how to _encode_ our unlabeled input $\\{\\mathbf{x}_{i}\\}_{i\\in[1,n]}$ and to _decode_ the corresponding representation. This idea give rise to the notion of **auto-encoder**. \n",
    "\n",
    "### Architecture \n",
    "\n",
    "The auto-encoder is an unsupervised architecture originally proposed to perform _dimensionality reduction_ [[3](#reference3)]. As its name indicates, we will try to train this model to learn an efficient _encoding_ $\\mathbf{z}$ of unlabeled input data $\\mathbf{x}$. The only way to learn efficient parameters is to also learn a _decoding_ function to _reconstruct_ $\\mathbf{x}$ from $\\mathbf{z}$.\n",
    "\n",
    "<img src=\"images/auto_encoder.png\"/>\n",
    "\n",
    "As shown here, a first model $\\mathcal{E}_\\phi$ _encodes_ the input into a _latent code_ $\\mathbf{z}$ in order to provide a low-dimensional representation of the data. A second model $\\mathcal{D}_\\theta$ designated as the _decoder_ aims to generate outputs from $\\mathbf{z}$ that are as close to the original inputs as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal definition\n",
    "\n",
    "The latent code $\\mathbf{z}$ can be seen as a compressed abstract representation, and may be used as an intermediate space for analysis or generation. This helps to govern the distribution of the data through a simpler and higher-level representation, while enhancing the _expressiveness_ of the generative model.\n",
    "The behaviour of an auto-encoder can be formalized as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z} &= \\mathcal{E}_\\phi(\\mathbf{x}) \\\\\n",
    "\\mathbf{\\hat{x}} &= \\mathcal{D}_\\theta(\\mathbf{z})  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with the _encoder_ $\\mathcal{E}_\\phi$ and _decoder_ $\\mathcal{D}_\\theta$ functions parameterized respectively by $\\phi$ and $\\theta$. As we can see this defines the reconstruction relationship\n",
    "$$\n",
    "    \\mathbf{\\hat{x}} = \\mathcal{D}_\\theta(\\mathcal{E}_\\phi(\\mathbf{x}))  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The training of an auto-encoder consists in finding the optimal functions of encoding $\\mathcal{E}^*$ and decoding $\\mathcal{D}^*$ by evaluating the \\textit{reconstruction error} $\\mathcal{L}$ between $\\mathbf{x}$ and $\\mathbf{\\hat{x}}$, such that\n",
    "\\begin{equation}\n",
    "    \\mathcal{E}^*, \\mathcal{D}^* = arg\\,min_{ \\phi, \\theta}{\\mathcal{L}}(\\mathbf{x}, \\mathcal{D}_\\theta(\\mathcal{E}_\\phi(\\mathbf{x})))\n",
    "\\end{equation}\n",
    "\n",
    "As the latent space usually has a smaller dimensionality than the input, it acts as an incentive for the network to find the main attributes of variations in the dataset (and also explains its use for _dimensionality reduction_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants and discussion\n",
    "\n",
    "There are several variants of auto-encoders, such as denoising auto-encoders or variational auto-encoders. Each address some downside of the basic AE model. For instance, the deterministic nature of the basic auto-encoder implies a point-wise mapping of the latent space, meaning that not all the latent positions can be leveraged to produce relevant reconstructions. Because of this reason, there is no way to ensure that the latent space could allow a robust generalization and that any random $\\mathbf{z}$ would generate a meaningful output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAxMjUCBUa43"
   },
   "source": [
    "<a id=\"implement\"> </a>\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Here, we discuss how we can implement and train a simple auto-encoder network in Pytorch. As discussed earlier, an AE is composed of two parts, an **encoder** and a **decoder**. The goal of the encoder is to \"compress\" the dataset, representing its principal features with a very small code, while the goal of the decoder is to learn how to reproduce the initial input from this code. Hence, we will first need to use some basic imports and definition to setup our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "### Import Pytorch and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YfIk2es3hJEd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 15:53:56.440740: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/envs/creative_ml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {'ace': '//cdnjs.cloudflare.com/ajax/libs/ace/1.4.7', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'ace/ext-language_tools': {'deps': ['ace/ace']}, 'ace/ext-modelist': {'deps': ['ace/ace']}, 'gridstack': {'exports': 'GridStack'}}});\n",
       "      require([\"ace/ace\"], function(ace) {\n",
       "\twindow.ace = ace\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"ace/ext-language_tools\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"ace/ext-modelist\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"gridstack\"], function(GridStack) {\n",
       "\twindow.GridStack = GridStack\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"notyf\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      root._bokeh_is_loading = css_urls.length + 5;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n",
       "    }    if (((window['ace'] !== undefined) && (!(window['ace'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ace.js', 'https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-language_tools.js', 'https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-modelist.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/gridstack/gridstack@4.2.5/dist/gridstack-h5.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) >= 0) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ace.js\", \"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-language_tools.js\", \"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-modelist.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var css_urls = [\"https://cdn.holoviz.org/panel/0.14.4/dist/css/debugger.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/alerts.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/card.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/widgets.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/markdown.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/json.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/loading.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/dataframe.css\"];\n",
       "  var inline_js = [    function(Bokeh) {\n",
       "      inject_raw_css(\"/*\\n ~ CML // Creative Machine Learning ~\\n mml.css : CSS styling information for Panel and Bokeh\\n \\n This file defines the main CSS styling information for the CML course\\n \\n Author               :  Philippe Esling\\n                        <esling@ircam.fr>\\n*/\\n\\nbody {\\n  display: flex;\\n  height: 100vh;\\n  margin: 0px;\\n  overflow-x: hidden;\\n  overflow-y: hidden;\\n}\\n\\n.bk-root .bk, .bk-root .bk:before, .bk-root .bk:after {\\n  font-family: \\\"Josefin Sans\\\";\\n}\\n\\nimg {\\n  max-width: 100%;\\n}\\n\\n#container {\\n  padding:0px;\\n  height:100vh;\\n  width: 100vw;\\n  max-width: 100vw;\\n}\\n\\n#sidebar .mdc-list {\\n  padding-left: 5px;\\n  padding-right: 5px;\\n}\\n\\n.mdc-drawer-app-content {\\n  flex: auto;\\n  position: relative;\\n  overflow: hidden;\\n}\\n\\n.mdc-drawer {\\n  background: #FAFAFA; /* GRAY 50 */\\n}\\n\\n.mdc-drawer-app-content {\\n  margin-left: 0 !important;\\n}\\n\\n.title-bar {\\n  display: contents;\\n  justify-content: center;\\n  align-content: center;\\n  width: 100%;\\n}\\n\\n.mdc-top-app-bar .bk-menu {\\n  color: black\\n}\\n\\n.app-header {\\n  display: contents;\\n  padding-left: 10px;\\n  font-size: 1.25em;\\n}\\n\\nimg.app-logo {\\n  padding-right: 10px;\\n  font-size: 28px;\\n  height: 30px;\\n  max-width: inherit;\\n  padding-top: 12px;\\n  padding-bottom: 6px;\\n}\\n\\n#app-title {\\n  padding-right: 12px;\\n  padding-left: 12px;\\n}\\n\\n.title {\\n  font-family: \\\"Josefin Sans\\\";\\n  color: #fff;\\n  text-decoration: none;\\n  text-decoration-line: none;\\n  text-decoration-style: initial;\\n  text-decoration-color: initial;\\n  font-weight: 400;\\n  font-size: 2em;\\n  line-height: 2em;\\n  white-space: nowrap;\\n}\\n\\n.main-content {\\n  overflow-y: scroll;\\n  overflow-x: auto;\\n}\\n\\n#header {\\n  position: absolute;\\n  z-index: 7;\\n}\\n\\n#header-items {\\n  width: 100%;\\n  margin-left:15px;\\n}\\n\\n.pn-busy-container {\\n  align-items: center;\\n  justify-content: center;\\n  display: flex;\\n}\\n\\n.mdc-drawer__content {\\n  overflow-x: hidden;\\n}\\n.mdc-drawer__content, .main-content {\\n  padding: 12px;\\n}\\n\\n.main-content {\\n  height: calc(100vh - 88px);\\n  max-height: calc(100vh - 88px);\\n  padding-right: 32px;\\n}\\n\\nbutton.mdc-button.mdc-card-button {\\n  color: transparent;\\n  height: 50px;\\n}\\n\\np.bk.mdc-button {\\n  display: none;\\n}\\n\\ndiv.bk.mdc-card {\\n  border-radius: 0px\\n}\\n\\n.mdc-card .bk.card-header {\\n  display: flex;\\n}\\n\\n.bk.mdc-card-title {\\n  font-family: \\\"Josefin Sans\\\";\\n  font-weight: bold;\\n  align-items: center;\\n  display: flex !important;\\n  position: relative !important;\\n}\\n\\n.bk.mdc-card-title:nth-child(2) {\\n  margin-left: -1.4em;\\n}\\n\\n.pn-modal {\\n  overflow-y: scroll;\\n  width: 100%;\\n  display: none;\\n  position: absolute;\\n  top: 0;\\n  left: 0;\\n}\\n\\n.pn-modal-content {\\n  font-family: \\\"Josefin Sans\\\";\\n  background-color: #0e0e0e;\\n  margin: auto;\\n  margin-top: 25px;\\n  margin-bottom: 25px;\\n  padding: 15px 20px 20px 20px;\\n  border: 1px solid #888;\\n  width: 80% !important;\\n}\\n\\n.pn-modal-close {\\n  position: absolute;\\n  right: 25px;\\n  z-index: 100;\\n}\\n\\n.pn-modal-close:hover,\\n.pn-modal-close:focus {\\n  color: #000;\\n  text-decoration: none;\\n  cursor: pointer;\\n}\\n\\n.custom_button_bokeh button.bk.bk-btn.bk-btn-default {\\n    font-size:48pt;\\n    background-color: #05b7ff;\\n    border-color: #05b7ff;\\n}\");\n",
       "    },    function(Bokeh) {\n",
       "      inject_raw_css(\"\\n    .bk.pn-loading.arc:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n",
       "    },    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, js_modules, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'ace': '//cdnjs.cloudflare.com/ajax/libs/ace/1.4.7', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'ace/ext-language_tools': {'deps': ['ace/ace']}, 'ace/ext-modelist': {'deps': ['ace/ace']}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"ace/ace\"], function(ace) {\n\twindow.ace = ace\n\ton_load()\n      })\n      require([\"ace/ext-language_tools\"], function() {\n\ton_load()\n      })\n      require([\"ace/ext-modelist\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 5;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n    }    if (((window['ace'] !== undefined) && (!(window['ace'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ace.js', 'https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-language_tools.js', 'https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-modelist.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/gridstack/gridstack@4.2.5/dist/gridstack-h5.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ace.js\", \"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-language_tools.js\", \"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-modelist.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var css_urls = [\"https://cdn.holoviz.org/panel/0.14.4/dist/css/debugger.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/alerts.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/card.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/widgets.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/markdown.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/json.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/loading.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/dataframe.css\"];\n  var inline_js = [    function(Bokeh) {\n      inject_raw_css(\"/*\\n ~ CML // Creative Machine Learning ~\\n mml.css : CSS styling information for Panel and Bokeh\\n \\n This file defines the main CSS styling information for the CML course\\n \\n Author               :  Philippe Esling\\n                        <esling@ircam.fr>\\n*/\\n\\nbody {\\n  display: flex;\\n  height: 100vh;\\n  margin: 0px;\\n  overflow-x: hidden;\\n  overflow-y: hidden;\\n}\\n\\n.bk-root .bk, .bk-root .bk:before, .bk-root .bk:after {\\n  font-family: \\\"Josefin Sans\\\";\\n}\\n\\nimg {\\n  max-width: 100%;\\n}\\n\\n#container {\\n  padding:0px;\\n  height:100vh;\\n  width: 100vw;\\n  max-width: 100vw;\\n}\\n\\n#sidebar .mdc-list {\\n  padding-left: 5px;\\n  padding-right: 5px;\\n}\\n\\n.mdc-drawer-app-content {\\n  flex: auto;\\n  position: relative;\\n  overflow: hidden;\\n}\\n\\n.mdc-drawer {\\n  background: #FAFAFA; /* GRAY 50 */\\n}\\n\\n.mdc-drawer-app-content {\\n  margin-left: 0 !important;\\n}\\n\\n.title-bar {\\n  display: contents;\\n  justify-content: center;\\n  align-content: center;\\n  width: 100%;\\n}\\n\\n.mdc-top-app-bar .bk-menu {\\n  color: black\\n}\\n\\n.app-header {\\n  display: contents;\\n  padding-left: 10px;\\n  font-size: 1.25em;\\n}\\n\\nimg.app-logo {\\n  padding-right: 10px;\\n  font-size: 28px;\\n  height: 30px;\\n  max-width: inherit;\\n  padding-top: 12px;\\n  padding-bottom: 6px;\\n}\\n\\n#app-title {\\n  padding-right: 12px;\\n  padding-left: 12px;\\n}\\n\\n.title {\\n  font-family: \\\"Josefin Sans\\\";\\n  color: #fff;\\n  text-decoration: none;\\n  text-decoration-line: none;\\n  text-decoration-style: initial;\\n  text-decoration-color: initial;\\n  font-weight: 400;\\n  font-size: 2em;\\n  line-height: 2em;\\n  white-space: nowrap;\\n}\\n\\n.main-content {\\n  overflow-y: scroll;\\n  overflow-x: auto;\\n}\\n\\n#header {\\n  position: absolute;\\n  z-index: 7;\\n}\\n\\n#header-items {\\n  width: 100%;\\n  margin-left:15px;\\n}\\n\\n.pn-busy-container {\\n  align-items: center;\\n  justify-content: center;\\n  display: flex;\\n}\\n\\n.mdc-drawer__content {\\n  overflow-x: hidden;\\n}\\n.mdc-drawer__content, .main-content {\\n  padding: 12px;\\n}\\n\\n.main-content {\\n  height: calc(100vh - 88px);\\n  max-height: calc(100vh - 88px);\\n  padding-right: 32px;\\n}\\n\\nbutton.mdc-button.mdc-card-button {\\n  color: transparent;\\n  height: 50px;\\n}\\n\\np.bk.mdc-button {\\n  display: none;\\n}\\n\\ndiv.bk.mdc-card {\\n  border-radius: 0px\\n}\\n\\n.mdc-card .bk.card-header {\\n  display: flex;\\n}\\n\\n.bk.mdc-card-title {\\n  font-family: \\\"Josefin Sans\\\";\\n  font-weight: bold;\\n  align-items: center;\\n  display: flex !important;\\n  position: relative !important;\\n}\\n\\n.bk.mdc-card-title:nth-child(2) {\\n  margin-left: -1.4em;\\n}\\n\\n.pn-modal {\\n  overflow-y: scroll;\\n  width: 100%;\\n  display: none;\\n  position: absolute;\\n  top: 0;\\n  left: 0;\\n}\\n\\n.pn-modal-content {\\n  font-family: \\\"Josefin Sans\\\";\\n  background-color: #0e0e0e;\\n  margin: auto;\\n  margin-top: 25px;\\n  margin-bottom: 25px;\\n  padding: 15px 20px 20px 20px;\\n  border: 1px solid #888;\\n  width: 80% !important;\\n}\\n\\n.pn-modal-close {\\n  position: absolute;\\n  right: 25px;\\n  z-index: 100;\\n}\\n\\n.pn-modal-close:hover,\\n.pn-modal-close:focus {\\n  color: #000;\\n  text-decoration: none;\\n  cursor: pointer;\\n}\\n\\n.custom_button_bokeh button.bk.bk-btn.bk-btn-default {\\n    font-size:48pt;\\n    background-color: #05b7ff;\\n    border-color: #05b7ff;\\n}\");\n    },    function(Bokeh) {\n      inject_raw_css(\"\\n    .bk.pn-loading.arc:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n    },    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, js_modules, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.bk-root, .bk-root .bk:before, .bk-root .bk:after {\n",
       "  font-family: var(--jp-ui-font-size1);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from cml.plot import initialize_bokeh\n",
    "from cml.panel import initialize_panel\n",
    "from jupyterthemes.stylefx import set_nb_theme\n",
    "from bokeh.io import show\n",
    "initialize_bokeh()\n",
    "initialize_panel()\n",
    "set_nb_theme(\"onedork\")\n",
    "rng = np.random.RandomState(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "### Load the dataset\n",
    "\n",
    "To start with a pragmatic and simple to understand example, we  will try to train the basic AE using the Fashion MNIST dataset. This dataset contains images of size 28x28 pixels, with different pieces of clothing. The following code allows to load (and eventually download) the dataset, by using the `torchvision.datasets` module. We also plot some randomly selected test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YZm503-I_tji"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "dataset_dir = './data'\n",
    "# Going to use 80%/20% split for train/valid\n",
    "valid_ratio = 0.2\n",
    "# Load the dataset for the training/validation sets\n",
    "train_valid_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir, train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "# Split it into training and validation sets\n",
    "nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\n",
    "nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
    "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
    "# Load the test set\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir, transform=torchvision.transforms.ToTensor(),train=False)\n",
    "# Prepare \n",
    "num_threads = 4     # Loading the dataset is using 4 CPU threads\n",
    "batch_size  = 128   # Using minibatches of 128 samples\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_threads)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_threads)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False,num_workers=num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to check the properties of our different sets and also plot some random examples, in order to better understand what type of data we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set contains 48000 images, in 375 batches\n",
      "The validation set contains 12000 images, in 94 batches\n",
      "The test set contains 10000 images, in 79 batches\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The train set contains {} images, in {} batches\".format(len(train_loader.dataset), len(train_loader)))\n",
    "print(\"The validation set contains {} images, in {} batches\".format(len(valid_loader.dataset), len(valid_loader)))\n",
    "print(\"The test set contains {} images, in {} batches\".format(len(test_loader.dataset), len(test_loader)))\n",
    "nsamples = 10\n",
    "classes_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal','Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "imgs_test, labels = next(iter(train_loader))\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(nsamples):\n",
    "    ax = plt.subplot(1,nsamples, i+1)\n",
    "    plt.imshow(imgs_test[i, 0, :, :], vmin=0, vmax=1.0, cmap=matplotlib.cm.gray)\n",
    "    ax.set_title(\"{}\".format(classes_names[labels[i]]), fontsize=15)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(imgs_test[i, 0, :, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"architecture\"></a>\n",
    "\n",
    "### Define the deep autoencoding architecture\n",
    "\n",
    "First, we will define the deep autoencoder architecture as a guideline to what we are aiming to train by creating a custom nn.Module class. The autoencoder will have multiple hidden layers in both the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"pretraining\"></a>\n",
    "### Layer-wise pretraining\n",
    "\n",
    "In this section, we will perform layer-wise pretraining of the deep autoencoder. We will train each layer individually as a shallow autoencoder, with its own encoder and decoder.\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 1.1 - Implementing the **layer-wise pretraining**\n",
    "\n",
    "> 1. Complete the `pretrain_layer` function that performs the pretraining of a given layer\n",
    ">    * Define the architecture of a single autoencoder\n",
    ">    * Create a criterion and optimizer\n",
    ">    * **Obtain the encoded features from previously trained layers**\n",
    "> 2. Update the training loop to learn the weights of both layers.\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining layer 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_sizes)):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretraining layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m     pretrained_layer \u001b[38;5;241m=\u001b[39m pretrain_layer(pretrained_layers, i, input_sizes[i], output_sizes[i], \u001b[43mtrain_loader\u001b[49m)\n\u001b[1;32m     48\u001b[0m     pretrained_layers\u001b[38;5;241m.\u001b[39mappend(pretrained_layer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function for layer-wise pretraining\n",
    "def pretrain_layer(pretrained_layers, layer_id, layer_in, layer_out, train_loader, num_epochs=50):\n",
    "    \n",
    "    # Define the shallow autoencoder architecture\n",
    "    shallow_autoencoder = nn.Sequential(\n",
    "        nn.Linear(layer_in, layer_out),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(layer_out, layer_in),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    # Our training criterion and optimizer\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.SGD(shallow_autoencoder.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_features, _ in train_loader:\n",
    "            # Eventually pass the batch through our previous layers\n",
    "            batch_encoded = batch_features.view(128,-1)\n",
    "            for i in range(layer_id): \n",
    "                batch_encoded = pretrained_layers[i].forward(batch_encoded) \n",
    "            # Training loop for a given layer\n",
    "            ...\n",
    "            x = batch_encoded\n",
    "            y_pred = shallow_autoencoder.forward(x)\n",
    "            \n",
    "            loss = criterion(y_pred, x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        # Print the corresponding advances\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Return the trained shallow autoencoder\n",
    "    return shallow_autoencoder\n",
    "\n",
    "# Pretrain each layer of the autoencoder\n",
    "pretrained_layers = []\n",
    "input_sizes = [28 * 28, 128, 128, 64, 64]\n",
    "output_sizes = [128, 128, 64, 64, 32]\n",
    "for i in range(len(input_sizes)):\n",
    "    print(f\"Pretraining layer {i+1}\")\n",
    "    pretrained_layer = pretrain_layer(pretrained_layers, i, input_sizes[i], output_sizes[i], train_loader)\n",
    "    pretrained_layers.append(pretrained_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"finetuning\"></a>\n",
    "### Fine-tuning the Autoencoder\n",
    "\n",
    "After layer-wise pretraining, we will fine-tune the entire deep autoencoder using the pretrained layers' weights. Note that what we have right now is a set of pretrained layers contained in the `pretrained_layers` array. However, we still need to connect these layers together by **transferring their weights to our full deep autoencoder architecture**.\n",
    "\n",
    "If you check the documentation and code for the `nn.Linear` layer, you can see that the actual weights are stored inside two Tensors respectively named\n",
    "- `layer.weight.data`\n",
    "- `layer.bias.data`\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 1.2 - Finetuning the autoencoder\n",
    "\n",
    "> 1. Transfer the weights from the pretrained layers to the full architecture\n",
    "> 2. Fill in the training criterion and optimizer\n",
    "> 2. Finish the training loop to finetune your deep AE\n",
    "  \n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the deep autoencoder with pretrained weights\n",
    "autoencoder = DeepAutoencoder().to(device)\n",
    "# Load pretrained weights into the deep autoencoder\n",
    "for i, layer in enumerate(autoencoder.encoder):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        ######################\n",
    "        layer.weight.data = pretrained_layer[i][0].weight.data\n",
    "        layer.bias.data = pretrained_layer[i][0].bias.data\n",
    "\n",
    "for i, layer in enumerate(autoencoder.decoder):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        layer.weight.data = pretrained_layer[i][2].weight.data\n",
    "        layer.bias.data = pretrained_layer[i][2].bias.data\n",
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "# Fine-tune the deep autoencoder\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.SGD(autoencoder.parameters(), lr=1e-4)\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_features, _ in train_loader:\n",
    "        \n",
    "        x = batch_features.view(-1, 784)\n",
    "        \n",
    "        y_pred = autoencoder.forward(x)\n",
    "        \n",
    "        loss = criterion(y_pred, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"visualization\"></a>\n",
    "### Visualizing the Results\n",
    "\n",
    "First, we can visualize the results by comparing input images and their corresponding reconstructed images produced by the deep autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display the input and reconstructed images\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_features, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_loader\u001b[49m:\n\u001b[1;32m     15\u001b[0m     batch_features \u001b[38;5;241m=\u001b[39m batch_features\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m28\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m autoencoder(batch_features)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to display images\n",
    "def display_images(images, title):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(20, 2))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Display the input and reconstructed images\n",
    "for batch_features, _ in test_loader:\n",
    "    batch_features = batch_features.view(-1, 28 * 28).to(device)\n",
    "    outputs = autoencoder(batch_features)\n",
    "    input_images = batch_features.cpu().detach().numpy()\n",
    "    reconstructed_images = outputs.cpu().detach().numpy()\n",
    "    display_images(input_images, 'Input Images')\n",
    "    display_images(reconstructed_images, 'Reconstructed Images')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"finetuning\"></a>\n",
    "### Using the encoder for supervised classification\n",
    "\n",
    "The following question is entirely open, without any specific code guideline to follow. This is aimed at exercising your own problem-solving skills in order to define the code architecture by yourself. Now that we have a finetuned auto-encoder, we would like to use the _encoding_ part for _supervised classification_. As seen in class, this amounts to a form of _transfer learning_. Hence, we need to retrieve the encoder and add extra layers that we will train in a supervised manner (in order to recognize the class of different fashion items).\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 1.3 - Transfer learning for classification\n",
    "\n",
    "> 1. Define the architecture of your target classifier\n",
    "> 2. Transfer the weights from our finetuned encoder\n",
    "> 3. Define the complete training of this classifier\n",
    "> 4. Evaluate your solution\n",
    "  \n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep autoencoders using JAX\n",
    "\n",
    "The following is an **optional exercise** in which you can try to implement the same mechanisms in JAX. To speed up the implementation, we propose to rely on the Flax library, which allows an easy definition of models.\n",
    "\n",
    "### Setup and Dataset Preparation\n",
    "\n",
    "First, we need to import the necessary JAX libraries. Note that we will reuse the same dataset as previously, but you will need to _cast the batches_ into a JAX-compliant format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "import optax\n",
    "import flax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"architecture\"></a>\n",
    "### Autoencoder training\n",
    "\n",
    "Next, we will define the deep autoencoder architecture using the Flax library in JAX.\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Exercice (optional)\n",
    ">   1. Define your architecture using Flax.\n",
    ">   2. Perform the layerwise pretraining\n",
    ">   3. Transfer the weights to a deep architecture\n",
    ">   4. Finetune and evaluate your model\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        ...\n",
    "        \n",
    "\n",
    "# Encoder\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        ...\n",
    "        \n",
    "\n",
    "# Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        ...\n",
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "# Layerwise pretraining\n",
    "...\n",
    "\n",
    "# Model finetuning\n",
    "...\n",
    "\n",
    "# Model evaluation\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEdCXSwCoKok"
   },
   "source": [
    "# Generative auto-encoder (modern version)\n",
    "\n",
    "As seen in the course, the previous training version of AEs was used for several years, but several advances in **model regularization** now allows to bypass the need for layer-wise training and directly train deep version of the AEs. These advances are _batch normalization_, _ReLU_ activations and _Dropout_. Furthermore, by definition, the AE can already be seen as a form of _generative model_ (although it lacks a probabilistic definition). \n",
    "\n",
    "Therefore, we will define in the following a simple AE seen as a generative model, to directly learn how to compress and reconstruct images from simple small vectors. Hence, we will need an `encoder`, which compresses the images into a small latent vector, and a `decoder`, that reconstructs the original image from this code. Here, we will start very basic and define the encoder and decoder as simple `Linear` layers. Note that we also use `BatchNorm`, `ReLU` and `Dropout` to regularize our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(AE, self).__init__()\n",
    "        self.latent_dim = encoding_dim\n",
    "        self.hidden_dim = 256\n",
    "        self.encoder = nn.Sequential(\n",
    "          nn.Linear(28 * 28, self.hidden_dim),\n",
    "          nn.BatchNorm1d(self.hidden_dim), nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(self.hidden_dim, self.hidden_dim), \n",
    "          nn.BatchNorm1d(self.hidden_dim), nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(self.hidden_dim, self.latent_dim), nn.Sigmoid()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.Linear(self.latent_dim, self.hidden_dim), \n",
    "          nn.BatchNorm1d(self.hidden_dim), nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(self.hidden_dim, self.hidden_dim), \n",
    "          nn.BatchNorm1d(self.hidden_dim), nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(self.hidden_dim, 28 * 28), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x.reshape(-1, 28 * 28))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the model depends on a given `encoding_dim` variable, which defines the size of the latent code. Therefore, we can instantiate our model arbitrarliy with `64` dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MUxidpyChjX"
   },
   "outputs": [],
   "source": [
    "latent_dim = 64   \n",
    "model = AE(latent_dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only remaining part that we did not discuss yet is what type of _loss_ (defined as $\\mathcal{L}$) we can use to train our model. First, we will simply rely on the _Mean Squared Error_ (MSE) loss, which is defined as\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_{MSE}(\\hat{\\mathbf{x}}, \\mathbf{x}) = \\mid \\hat{\\mathbf{x}}, \\mathbf{x} \\mid^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9I1JlqEIDCI4"
   },
   "outputs": [],
   "source": [
    "# Loss function that we will use\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oJSeMTroABs"
   },
   "source": [
    "<a id=\"application\"> </a>\n",
    "\n",
    "### Training the model\n",
    "\n",
    "Train the model using `x_train` as both the input and the target. The `encoder` will learn to compress the dataset from 784 dimensions to the latent space, and the `decoder` will learn to reconstruct the original images.\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1RI9OfHDBsK"
   },
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 1e-4\n",
    "# Optimizer to fit the weights of the network\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for t in range(20):\n",
    "    full_loss = torch.Tensor([0])\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        y_pred = model(x)\n",
    "        # Compute the loss.\n",
    "        loss = loss_fn(y_pred, x)\n",
    "        # Before the backward pass, zero all of the network gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass: compute gradient of the loss with respect to parameters\n",
    "        loss.backward()\n",
    "        # Calling the step function to update the parameters\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAM1QBhtoC-n"
   },
   "source": [
    "Now that the model is trained, we can test it by encoding and decoding images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pbr5WCj7FQUi"
   },
   "outputs": [],
   "source": [
    "encoded_imgs = model.encoder(imgs_test.reshape(-1,28*28))\n",
    "decoded_imgs = model.decoder(encoded_imgs).reshape(-1,28,28).detach().numpy()\n",
    "\n",
    "#decoded_imgs = model(imgs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the images, we can see that the model is able to perform an adequate (yet somewhat blurry) reconstruction of the input images. The interesting point is that this reconstruction comes from a code of only `64` dimensions, whereas the original images have `784` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4LlDOS6FUA1"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(imgs_test[i, 0])\n",
    "    plt.title(\"original\"); plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i])\n",
    "    plt.title(\"reconstructed\"); plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "Even though this very basic example seems to work, several improvements can be made over the original model. First, we can see that the overall framework does not depend on the exact nature of the `encoder` and `decoder`. \n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Exercise (optional)\n",
    ">   1. Rewrite the original class to accept any type of architecture for these (see code below). \n",
    ">   2. Fill the missing code to have a new AE model\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, encoding_dim):\n",
    "        super(AE, self).__init__()\n",
    "        self.latent_dim = encoding_dim\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "\n",
    "encoder = ...\n",
    "decoder = ...\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we only used the train dataset, whereas the use of a validation and test sets allows to ensure that we do not overfit our model.\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Exercise (optional)\n",
    ">   1. Write a separate train function, that can be called on different sets\n",
    ">   2. Re-write the training loop to test overfitting\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4gv6G8PoRQE"
   },
   "source": [
    "## **Optional** exercise: Denoising AE\n",
    "\n",
    "Imagine (for the sake of argument), that we choose an encoding dimension which is of same dimensionality as the input one. Then, one huge problem is that nothing prevents the AE from simply learning the _identity_ function (try to imagine why). An autoencoder can also be trained to remove noise from images. This type of _regularization_ prevents the model from learning this degenerate situation.\n",
    "\n",
    "In this exercise, you will need to create your own denoising AE, by relying on a noisy version of the Fashion MNIST dataset (adding random Gaussian noise to each image). You will then train an autoencoder using the noisy image as input, and the original image as the target.\n",
    "\n",
    "Let's reimport the dataset to omit the modifications made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJZ-TcaqDBr5"
   },
   "outputs": [],
   "source": [
    "class GaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPZl_6P65_8R"
   },
   "source": [
    "Here, we create two new train and test sets by adding random noise to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axSMyxC354fc"
   },
   "outputs": [],
   "source": [
    "# Load the datasets and use our Gaussian noise transform\n",
    "train_valid_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir, train=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), GaussianNoise(0., 0.1)]), download=True)\n",
    "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), GaussianNoise(0., 0.1)]),train=False)\n",
    "# Create loaders\n",
    "train_loader_noisy = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_threads)\n",
    "valid_loader_noisy = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_threads)\n",
    "test_loader_noisy = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False,num_workers=num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRxHe4XXltNd"
   },
   "source": [
    "Plot the noisy images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thKUmbVVCQpt"
   },
   "outputs": [],
   "source": [
    "x_test, labels = next(iter(test_loader))\n",
    "x_test_noisy, labels = next(iter(test_loader_noisy))\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.title(\"original\")\n",
    "    plt.imshow(x_test[i].squeeze())\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.gray()\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.title(\"original + noise\")\n",
    "    plt.imshow(x_test_noisy[i].squeeze())\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.gray()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy9SY8jGl5aP"
   },
   "source": [
    "### Define a convolutional autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT_BhZngWMwp"
   },
   "source": [
    "In this example, you will train a convolutional autoencoder using  [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layers in the `encoder`, and [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) layers in the `decoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5KjoIlYCQko"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "\n",
    "class DenoisingAE(AE):\n",
    "    def __init__(self):\n",
    "        super(DenoisingAE, self).__init__()\n",
    "        self.encoder = ...    \n",
    "        self.decoder = ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = ...\n",
    "        decoded = ...\n",
    "        return decoded\n",
    "\n",
    "autoencoder = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the full optimization loop to optimize your denoising auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYKbiDFYCQfj"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7-VAuEy_N6M"
   },
   "source": [
    "Plot both the noisy images and the denoised images produced by the autoencoder to check that your implementation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfxr9NdBCP_x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "encoded_imgs = ...\n",
    "decoded_imgs = ...\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention layers and Transformers\n",
    "\n",
    "In the following exercise, we aim to implement attention layers and Transformers in PyTorch. Transformers are a powerful deep learning architecture that has been used to achieve state-of-the-art results in various natural language processing tasks. \n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #410819; border-color: #cb2e47\">\n",
    "\n",
    "> **Warning on the exercise difficulty and goal**\n",
    "\n",
    "> **Note that this exercise is a lot more involved, as we now move on to an almost research-level type of exercise, where we mostly provide a research paper as reference. Your goal is to try to reimplement the architecture and method proposed in that paper as closely as possible.**\n",
    "\n",
    "#### Paper reference\n",
    "\n",
    "[Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "**Link:** [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "<a name=\"intro\"></a>\n",
    "## Introduction to attention\n",
    "\n",
    "Attention mechanisms enable models to focus on specific parts of the input sequence while processing it. They have been proven to be highly effective in natural language processing tasks such as machine translation and sentiment analysis. Transformers are a type of deep learning model that incorporates self-attention mechanisms. They consist of an encoder and a decoder, and utilize multi-head attention layers, positional encoding, and layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining our own attention layer\n",
    "\n",
    "In the following, we re-implement the *attention* layer, which is the basis of the infamous `Transformer` models.\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #410819; border-color: #cb2e47\">\n",
    "\n",
    "> This is defined in Section 3.2.1 (pages 3 and 4) and depicted in Figure 2 (left) of our [paper reference.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Exercise (**course**)\n",
    ">   1. Implement the simple attention layer as defined in the slides\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE \n",
    "        ######################\n",
    "        ...\n",
    "\n",
    "    def forward(self, X):\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE \n",
    "        ######################\n",
    "        ...\n",
    "    \n",
    "\n",
    "######################\n",
    "# Solution: \n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.mlp = nn.Linear(n_hidden, n_hidden)\n",
    "        self.u_w = nn.Parameter(torch.rand(n_hidden))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # get the hidden representation of the sequence\n",
    "        u_it = F.tanh(self.mlp(X))\n",
    "        # get attention weights for each timestep\n",
    "        alpha = F.softmax(torch.matmul(u_it, self.u_w), dim=1)\n",
    "        # get the weighted sum of the sequence\n",
    "        out = torch.sum(torch.matmul(alpha, X), dim=1)\n",
    "        return out, alpha\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"setup\"></a>\n",
    "## Setup and dataset\n",
    "\n",
    "First, let's import the necessary libraries and prepare the dataset for training. We will now rely on a text dataset, which is readily available in the `torchtext` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import Multi30k\n",
    "# Translate English to German\n",
    "language_pair = (\"en\", \"de\")\n",
    "# Import the text dataset\n",
    "train_iterator, valid_iterator, test_iterator = Multi30k(split=(\"train\", \"valid\", \"test\"), language_pair=language_pair)\n",
    "# Potentially use the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"mha\"></a>\n",
    "### Implementing the Multi-Head Attention layer\n",
    "\n",
    "The multi-head attention layer is a key component of the Transformer architecture. It allows the model to focus on different parts of the input sequence simultaneously.\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #410819; border-color: #cb2e47\">\n",
    "\n",
    "> This is defined in Section 3.2.2 (pages 4 and 5) and depicted in Figure 2 (right) of our [paper reference.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 2.1 - Implementing the multi-head attention layer\n",
    "\n",
    "> 1. Following the definitions in the paper, propose your implementation of the _multi-head attention layer_.\n",
    "  \n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE \n",
    "        ######################\n",
    "        ...\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE \n",
    "        ######################\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"transformer\"></a>\n",
    "### Building the Transformer architecture\n",
    "\n",
    "Now that we have implemented the multi-head attention layer, we can build the Transformer architecture. We'll need to create the encoder, decoder, and the final Transformer model.\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #410819; border-color: #cb2e47\">\n",
    "\n",
    "> The transformer architecture is defined in Section 3.1 (pages 2 and 3) and depicted in Figure 1 of our [paper reference.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). Also more details about the architecture are provided in subsequent sections 3.3 to 3.5.\n",
    "\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 2.2 - Implementing the Transformer architecture\n",
    "\n",
    "> 1. Following the definitions in the paper, propose your implementation of the _transformer architecture_.\n",
    "> 2. (Optional) Implement the other parts of the architecture (sections 3.3 to 3.5).\n",
    "  \n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        ...\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        ...\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=100):\n",
    "        ...\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=100):\n",
    "        ...\n",
    "\n",
    "# Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"training\"></a>\n",
    "### Training the Transformer Model\n",
    "\n",
    "After building the Transformer architecture, we can train the model using the dataset prepared in section 2. Note that this exercise is left optional, as this requires several notions not covered in the course yet (embeddings, positional encodings). However, this is of course more rewarding to see your model being trained :)\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Exercise (**optional**)\n",
    ">   1. Implement the training loop for the Transformer.\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Transformer model\n",
    "INPUT_DIM = ...\n",
    "OUTPUT_DIM = ...\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
    "\n",
    "model = Transformer(enc, dec, device).to(device)\n",
    "\n",
    "# Train the model\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    ...\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    ...\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    ...\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"evaluation\"></a>\n",
    "### Evaluating the Model\n",
    "\n",
    "After training the Transformer model, we can evaluate its performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('transformer-model.pt'))\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optional fun** - Style transfer \n",
    "\n",
    "Style transfer is the technique of applying the style of one image to the content of another, creating a new image with the desired content and style. The following exercise aims at replicating the results of the original [Neural style transfer paper](https://arxiv.org/abs/1508.06576) by Gatys et al. \n",
    "\n",
    "Note that the following tutorial is a plain simplification inspired by the great [tutorial available in the Pytorch documentation](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html). Hence, all credits goes back to the authors of the tutorial.\n",
    "\n",
    "## Introduction to Style Transfer\n",
    "\n",
    "Style transfer is a technique in computer vision and deep learning that enables the transfer of artistic style from one image (style image) to another image (content image) while retaining the original content. This is typically achieved using a pre-trained deep neural network, such as VGG-19, and optimizing the input image to minimize the content and style losses.\n",
    "\n",
    "<a name=\"setup\"></a>\n",
    "## Import images\n",
    "\n",
    "First, let's import the necessary libraries and prepare the images for style transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# Find our device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# URL to load our content image from\n",
    "url = \"https://thumbnailer.mixcloud.com/unsafe/300x300/extaudio/6/b/a/b/d576-35c3-48a9-a01e-7dc4f0255e62.jpg\"\n",
    "response = requests.get(url)\n",
    "content_image = np.array(Image.open(BytesIO(response.content)))\n",
    "# URL to load our style image from\n",
    "url = \"https://t4.ftcdn.net/jpg/02/44/21/17/360_F_244211780_VFoZhDiuxyWpnTalr0DFilyYqNokEoVZ.jpg\"\n",
    "response = requests.get(url)\n",
    "style_image = np.array(Image.open(BytesIO(response.content)))\n",
    "# Display content and style images\n",
    "plt.imshow(content_image)\n",
    "plt.show()\n",
    "plt.imshow(style_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model_loss\"></a>\n",
    "## Pre-trained Model and Loss Functions\n",
    "\n",
    "We will use the pre-trained VGG-19 model, and define the content and style loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import VGG19_Weights\n",
    "# Load pre-trained VGG-19 model\n",
    "vgg = models.vgg19(weights=VGG19_Weights.DEFAULT).features.to(device).eval()\n",
    "\n",
    "# Content and style loss functions\n",
    "class ContentLoss(nn.Module):\n",
    "    ...\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prepost\"></a>\n",
    "## Image Preprocessing and Postprocessing\n",
    "We need to define functions for preprocessing and postprocessing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "def preprocess(image, max_size=512, shape=None):\n",
    "    ...\n",
    "\n",
    "# Image postprocessing\n",
    "def postprocess(tensor):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"implementation\"></a>\n",
    "## Style Transfer Implementation\n",
    "\n",
    "Now that we have everything set up, we can implement the style transfer algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style transfer function\n",
    "def style_transfer(content_image, style_image, content_layers, style_layers, content_weight, style_weight, iterations=2000):\n",
    "    ...\n",
    "\n",
    "# Perform style transfer\n",
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "content_weight = 1\n",
    "style_weight = 1e6\n",
    "\n",
    "output_image = style_transfer(content_image, style_image, content_layers, style_layers, content_weight, style_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"visualization\"></a>\n",
    "## Visualization\n",
    "\n",
    "We can visualize the output image and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize output image\n",
    "plt.imshow(output_image)\n",
    "plt.show()\n",
    "\n",
    "# Save the output image\n",
    "output_image.save(\"path/to/output/image.jpg\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlow_3_Autoencoder_Dimensionality_Reduction.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
